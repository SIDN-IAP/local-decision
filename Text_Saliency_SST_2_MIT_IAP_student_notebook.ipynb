{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Saliency - SST-2 - MIT IAP student notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85uj9EFgT6Ji",
        "colab_type": "text"
      },
      "source": [
        "## **MIT IAP 2020: Structure and Interpretation of Deep Networks**\n",
        "\n",
        "### Lecture 2: Explaining Predictions\n",
        "#### _~ Methods for Understanding and Interpreting the Behaviour of Neural-NLP Models_\n",
        "\n",
        "| Sebastian Gehrmann & Mirac Suzgun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScgcdOjmS98F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Importing relevant libraries and dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext import data, datasets\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from numpy import linalg as LA\n",
        "# import seaborn as sns; sns.set()\n",
        "\n",
        "# Matplotlib plt default settings\n",
        "plt.style.use('default')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWqoEAGbTFTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GPU check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)\n",
        "\n",
        "## Hyperparameters\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahfZZpb7XiC4",
        "colab_type": "text"
      },
      "source": [
        "### 1) Set up the Data\n",
        "\n",
        "A neural netork can only handle numerical inputs. Since text is represented as a sequences of words, we first need to map the words to integers. Luckily, we can use ***torchtext*** to process the text for us. \n",
        "\n",
        "In this lab, we will use the SST-2 corpus. The corpus comprises movie reviews with one sentence per review. The goal is to classify whether a review is positive or negative. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClW5-PM8S_Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Download and processthe SST-2 corpus\n",
        "\n",
        "# Set up input/output as torchtext Field objects\n",
        "TEXT = data.Field(\n",
        "    lower=True, # Lowercase all text\n",
        "    batch_first=True) # order the tensor with batch as the first dimension  \n",
        "LABEL = data.Field(\n",
        "    sequential=False, # Do not tokenize the label (it is a single \"pos\"/\"neg\")\n",
        "    unk_token=None) # Do not add an \"unknown\" token (not needed for label)\n",
        "\n",
        "# Make splits for the dataset\n",
        "# Filter out examples that are neutral\n",
        "train_split, valid_split, test_split = datasets.SST.splits(\n",
        "    TEXT, \n",
        "    LABEL, \n",
        "    filter_pred=lambda ex: ex.label != 'neutral')\n",
        "\n",
        "# Build the vocabulary\n",
        "# This builds an index such that \"the\" -> 1, \"dog\" -> 2 etc. \n",
        "TEXT.build_vocab(train_split,)\n",
        "LABEL.build_vocab(train_split)\n",
        "\n",
        "# Make iterator for splits\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_split, valid_split, test_split), batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kPdXquFaOVO",
        "colab_type": "text"
      },
      "source": [
        "### 2) Check that the processing has worked\n",
        "\n",
        "We have sucessfully built an ***Iterator*** for our data. This is an iterable that spits out one batch at a time of processed data. The text is converted to sequences of integers that represent the words and the labels are 0's and 1's. \n",
        "\n",
        "Among others, the iterator also handles the padding -- padding adds special symbols to shorter sequences such that all items in a batch have the same length. Without it, we could not fit a batch into a matrix with fixed width. \n",
        "\n",
        "To check that everything is functioning as expected, let's see how it works: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjH7A45GTAv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Number of unique words in our input texts')\n",
        "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
        "print('Number of labels in our corpus')\n",
        "print('len(LABEL.vocab)', len(LABEL.vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvH_OtnlTCm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the first batch of the dataset\n",
        "batch = next(iter(train_iter))\n",
        "print('Size of text batch: {} examples of length {}.'.format(\n",
        "    *batch.text.shape))\n",
        "# Grab the very first example within the batch\n",
        "example_text = batch.text [0, :]\n",
        "print()\n",
        "print('The tokenized example has a length of {} and looks as follows'.format(\n",
        "    len(example_text)))\n",
        "print(example_text)\n",
        "print('The label for the example is:', batch.label[0])\n",
        "\n",
        "# We can easily convert it back to text like this: \n",
        "print()\n",
        "print('Converted back to string:', ' '.join(\n",
        "    [TEXT.vocab.itos[i] for i in example_text.tolist()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUs7GlhdfvM",
        "colab_type": "text"
      },
      "source": [
        "Let's build some helper functions to convert from string to id's and back"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcrZ3LDPTJVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the sentence given the indices\n",
        "def ind2sent(vals):\n",
        "    return (\" \".join([TEXT.vocab.itos[i.item()] for i in vals]))\n",
        "\n",
        "# Get the vectorized version of a sentence\n",
        "def sent2ind(sentence):\n",
        "    words = sentence.split (' ')\n",
        "    arr = [TEXT.vocab.stoi[word] for word in words]\n",
        "    return torch.tensor (arr).long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbFOmczKTKi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example sentence \n",
        "ind2sent (example_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEnzclBCjQ3j",
        "colab_type": "text"
      },
      "source": [
        "### Models\n",
        "\n",
        "We are now building a model that we can use to train the sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5UUJ7VFjO6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the index that represents the padding symbol\n",
        "PAD_IX = TEXT.vocab.stoi['<pad>']\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "# CNN setup\n",
        "N_FILTERS = 25\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# LSTM setup\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FrrOPOljzZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN (nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, n_filters, filter_sizes, dropout):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv2d(\n",
        "                in_channels = 1, \n",
        "                out_channels = n_filters, \n",
        "                kernel_size = (fs, embedding_dim)) \n",
        "             for fs in filter_sizes])\n",
        "        self.fc = nn.Linear(\n",
        "            len(filter_sizes) * n_filters, \n",
        "            output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, embedded):\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        # embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
        "        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        # pooled_n = [batch size, n_filters]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        output = self.fc(cat).sigmoid ().squeeze()\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE-f-USfTNWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eval\n",
        "def eval (model, test_iter = test_iter):\n",
        "    correct_preds = 0\n",
        "    total_examples = 0\n",
        "    \n",
        "    # Turn on the test mode\n",
        "    model.eval ()\n",
        "    with torch.no_grad():\n",
        "        for index, batch in enumerate(test_iter):\n",
        "            # Input and target\n",
        "            input = batch.text\n",
        "            target = batch.label.long()\n",
        "            # Feed the input to the model\n",
        "            predictions = model (embedding(input))\n",
        "            predictions = (predictions >= 0.5).long().squeeze()\n",
        "            \n",
        "            total_examples += len(target)\n",
        "            correct_preds += (predictions == target).sum().item()\n",
        "\n",
        "    return correct_preds/total_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fp68Aw4TOjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training loop\n",
        "def train(model, embedding, criterion, optim, n_epochs=5):\n",
        "    # Turn on the train mode\n",
        "    model.train()\n",
        "    for epoch in range (n_epochs):\n",
        "        # Set up progress bar\n",
        "        tracker =  tqdm(enumerate(train_iter), \n",
        "                                 total=len(train_iter),\n",
        "                                 desc='Epoch {}'.format(epoch+1))\n",
        "        for index, batch in tracker:\n",
        "            # Initialize the optimizer\n",
        "            optim.zero_grad()\n",
        "            \n",
        "            # Input and target\n",
        "            input = batch.text\n",
        "            target = batch.label.float()\n",
        "            \n",
        "            # Feed the input to the model\n",
        "            predictions = model(embedding(input))\n",
        "            \n",
        "            loss = criterion(predictions, target)\n",
        "            \n",
        "            # Perform updates in backpropogation\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            # Report the loss on progress bar every 10 batches\n",
        "            if index % 10 == 0:\n",
        "                tracker.set_postfix(loss=loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R3EMriVTQBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "# Embedding module setup \n",
        "embedding = nn.Embedding(\n",
        "    len(TEXT.vocab), \n",
        "    EMBEDDING_DIM, \n",
        "    padding_idx = PAD_IX).to(device)\n",
        "# CNN model\n",
        "model = CNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_FILTERS, \n",
        "            FILTER_SIZES, \n",
        "            DROPOUT).to(device)\n",
        "\n",
        "# Binary cross entropy loss\n",
        "loss = nn.BCELoss()\n",
        "# Learning rate\n",
        "learning_rate = 0.003\n",
        "# Adam optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(embedding.parameters()) + list(model.parameters()), \n",
        "    lr=learning_rate)\n",
        "\n",
        "print (model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwl3zkKOTRki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us the train the model\n",
        "EPOCH_NUM = 15\n",
        "train (model, embedding, loss, optimizer, EPOCH_NUM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYxifm16TTWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us evaluate the performance of the model on the test set\n",
        "eval(model, test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5nb2c_1oV_P",
        "colab_type": "text"
      },
      "source": [
        "### Compute the Saliency for Words\n",
        "\n",
        "We now have a functioning classifier and can start computing the importance of words in a sentence for a positive or negative review. \n",
        "\n",
        "One key difference to models in computer vision is that we have the mapping word -> word_id -> embedding. The embeddings receive the gradient updates, but are very high-dimensional. Therefore, the standard first-derivative saliency will yield EMEDDING_SIZE different importance numbers. Let's try this!\n",
        "\n",
        "### One note before we start: \n",
        "\n",
        "We may have noticed that we separated the embedding layer from the rest of the model. This is because we want to retain positional information for the saliency (i.e., where in a sentence each word is). If we did not do this, we could only access the gradient information of the whole embedding layer, but not of the embedding of each individual word. Let's see what happens in either case:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYnqQvLAthVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper to get a test example\n",
        "def get_example():\n",
        "    index = 0\n",
        "    batch = next(iter(train_iter))\n",
        "    print(\"Size of text batch:\", batch.text.shape)    \n",
        "    example_text = batch.text[index, :].view(1, -1)\n",
        "    # Truncate up to PADDING (but min 5 for CNN width)\n",
        "    example_text = example_text[:,:max(5,\n",
        "                                       (example_text[0] == PAD_IX).nonzero()[0])]\n",
        "\n",
        "    print(\"Size of text:\", example_text.shape)  \n",
        "    example_label = batch.label[index].view(-1, 1)\n",
        "\n",
        "    # Sentence\n",
        "    print(\"Next example:\", ind2sent(example_text[0]))\n",
        "    return example_text, example_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz8aJ-27oVTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex_in, ex_out = get_example()\n",
        "# zero the gradient\n",
        "model.zero_grad ()\n",
        "# Set up the embeddings as a variable\n",
        "emb = Variable(embedding(ex_in))\n",
        "emb.requires_grad = True\n",
        "pred = model(emb)\n",
        "pred.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePhZ-9QC7_1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrong: Here we access the gradient of the embedding module\n",
        "print(embedding.weight.grad.shape)\n",
        "# As you can see, all gradients are accumulated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLojymh9sYQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Correct: Here we access the gradient of \"emb\", the embeddings of the sentence, instead\n",
        "print(emb.grad.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUg1VSco8bhN",
        "colab_type": "text"
      },
      "source": [
        "#### Move from saliency per embedding dimension to saliency per word\n",
        "\n",
        "There are multiple possible ways to aggregate the gradient information. The most common one is to use the $L_2$-Norm. \n",
        "See [here](https://www.aclweb.org/anthology/N16-1082/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXteRuAI9J8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, let's remove the extra dimension\n",
        "# Then, we move the tensor to CPU to use numpy functions\n",
        "emb_data = emb.grad.clone().squeeze(0).cpu().numpy()\n",
        "emb_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yCyG76X8arz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, we can compute the L2-Norm per token\n",
        "LA.norm(emb_data, 2, 1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj2uLpCL-wVP",
        "colab_type": "text"
      },
      "source": [
        "Let's put this together now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBOA_X_m-u8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_l2_gradient_saliency(model, embedding, example_text):\n",
        "    # Turn off dropout\n",
        "    embedding.eval()\n",
        "    model.eval()\n",
        "    # zero the gradient\n",
        "    model.zero_grad()\n",
        "    # Set up the embeddings as a variable\n",
        "    emb = Variable(embedding(ex_in))\n",
        "    emb.requires_grad = True\n",
        "    pred = model(emb)\n",
        "    pred.backward()\n",
        "    # Move and reshape the gradient\n",
        "    emb_data = emb.grad.clone().squeeze(0).cpu().numpy()\n",
        "    # Compute the norm\n",
        "    l2_saliency = LA.norm(emb_data, 2, 1) \n",
        "    return l2_saliency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdss_A9X_P6J",
        "colab_type": "text"
      },
      "source": [
        "We can use this function to visualize the saliency! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjCUEO34-v7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_saliency(text, saliency):\n",
        "    overall_importance = np.expand_dims(saliency, axis=0)\n",
        "    plt.figure(figsize=(12, 1))\n",
        "    ax = sns.heatmap (overall_importance, \n",
        "                      xticklabels=ind2sent(text[0]).split(' '),\n",
        "                      yticklabels=[], \n",
        "                      cmap='coolwarm')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTxM0APqAT3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex_in, ex_out = get_example()\n",
        "saliency = compute_l2_gradient_saliency(model, embedding, ex_in)\n",
        "visualize_saliency(ex_in, saliency)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y9npdbeC_MJ",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Alternative Saliency\n",
        "\n",
        "An alternative way to aggregate the importance measure is to take the embedding itself into account. \n",
        "\n",
        "Please see this paper for reference: https://arxiv.org/abs/1906.10282 (Eq.3 should have all information you need).\n",
        "\n",
        "Fill in the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPJSzJKcC96o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_input_gradient_saliency(model, embedding, example_text):\n",
        "    # Fill in below\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx94NHA3Do6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this to test whether your function works\n",
        "ex_in, ex_out = get_example()\n",
        "saliency = compute_input_gradient_saliency(model, embedding, ex_in)\n",
        "visualize_saliency(ex_in, saliency)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk3pA4DXxyuy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Task 2: LSTM Saliency\n",
        "\n",
        "Try switching the model to an LSTM-based one. Does the Saliency still work? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRqo0ggPxy-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMClassifier (nn.Module):\n",
        "    def __init__(self, \n",
        "                 vocab_size, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 num_layers,\n",
        "                 output_dim,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.bilstm = nn.LSTM(input_size=embedding_dim, \n",
        "                              hidden_size=hidden_dim, \n",
        "                              num_layers=num_layers,\n",
        "                              dropout=dropout,\n",
        "                              bidirectional=True,\n",
        "                              batch_first=True)\n",
        "        self.fc = nn.Linear(\n",
        "            2 * hidden_dim, \n",
        "            output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # self.hidden = self.init_hidden()\n",
        "    \n",
        "    def init_hidden():\n",
        "        return (torch.zeros(2, self.batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(2, self.batch_size, self.hidden_dim).to(device))\n",
        "        \n",
        "    def forward(self, embedded):\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "        # lstm_out = [batch size, sent len, hid dim]\n",
        "        lstm_out, _ = self.bilstm(embedded, None)\n",
        "        lstm_out = self.dropout(lstm_out[:,-1,:])\n",
        "        output = self.fc(lstm_out).sigmoid().squeeze(-1)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMzdjAKZxzHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM module setup\n",
        "lstm_model = BiLSTMClassifier(\n",
        "    INPUT_DIM, \n",
        "    EMBEDDING_DIM, \n",
        "    HIDDEN_DIM,\n",
        "    NUM_LAYERS,\n",
        "    OUTPUT_DIM, \n",
        "    DROPOUT).to(device)\n",
        "\n",
        "# Embedding module setup \n",
        "lstm_embedding = nn.Embedding(\n",
        "    len(TEXT.vocab), \n",
        "    EMBEDDING_DIM, \n",
        "    padding_idx = PAD_IX).to(device)\n",
        "\n",
        "learning_rate = 0.005\n",
        "EPOCH_NUM = 30\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(lstm_embedding.parameters()) + list(lstm_model.parameters()), \n",
        "    lr=learning_rate)\n",
        "\n",
        "train(lstm_model, lstm_embedding, loss, optimizer, EPOCH_NUM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-s5Z4ya5InQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval(lstm_model, test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi4PlQzaU4OJ",
        "colab_type": "text"
      },
      "source": [
        "Now it's your turn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLjcjaOM8Jq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saliency\n",
        "\n",
        "# Compare the Saliency scores between the LSTM and the CNN models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fijsuJT4Du5H",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Extend the Saliency\n",
        "\n",
        "Above, we had to create a separate module for embeddings to retrieve the gradients per word. However, there is a smarter way to do this. This way involves `hooks`. There are forward and backward hooks in PyTorch that get called during the forward/backward passes through a network. During the backward pass, we can access the gradient input and output of a module. \n",
        "\n",
        "Let's rewrite our code from above to compute saliency for the CNN model: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ksbX5BjNrdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"example length:\", ex_in.shape[1])\n",
        "# Turn off dropout\n",
        "embedding.eval()\n",
        "model.eval()\n",
        "\n",
        "# define hook\n",
        "def backHook(module, gradInput, gradOutput):\n",
        "    print(\"Gradient Input:\", gradInput[0].shape)\n",
        "    print(\"Gradient Output:\", gradOutput[0].shape)\n",
        "    print(gradOutput[0][0][0][:10])\n",
        "    # Recompute the norm in here: \n",
        "    emb_data = gradOutput[0].clone().squeeze(0).cpu().numpy()\n",
        "    l2_saliency = LA.norm(emb_data, 2, 1) \n",
        "    print(l2_saliency)\n",
        "\n",
        "# Register the hook in the embedding\n",
        "handle = embedding.register_backward_hook(backHook)\n",
        "# zero the gradient\n",
        "model.zero_grad()\n",
        "# We no longer have to set up embedding as a Variable!\n",
        "pred = model(embedding(ex_in))\n",
        "print(\"PRED\", pred.item(), \"!!!!!!!\")\n",
        "print()\n",
        "pred.backward()\n",
        "# Remove the hook\n",
        "handle.remove()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRVNKpA4OrTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check that we get the same result the other way\n",
        "saliency = compute_l2_gradient_saliency(model, embedding, ex_in)\n",
        "saliency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPYW-u7PQ0E",
        "colab_type": "text"
      },
      "source": [
        "As you can see, hooks give us a much more powerful way to analyze the gradient information within a network. Let's try it!\n",
        "\n",
        "a) The first classifier is a CNN-based model. That means that it can take multiple words into account at once. Extend the Saliency computation below to CNN filters and find out which ***phrases*** are the most relevant for an input, instead of individual words. \n",
        "\n",
        "b) Find the overall most salient phrases across the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx4jni7qisFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}